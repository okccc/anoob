### install
```shell script
# es/kibana下载地址 https://www.elastic.co/cn/downloads/?elektra=home&storm=hero
# postman下载地址 https://www.postman.com/
# ik分词器下载地址 https://github.com/medcl/elasticsearch-analysis-ik/releases
# 安装es
[root@cdh1 ~]$ tar -xvf elasticsearch-7.9.0.tar.gz -C /opt/module/
# 查看版本,kibana和ik分词器要和es保持版本一致
[root@cdh1 ~]$ elasticsearch --version
Version: 7.9.0, Build: default/tar/a479a2a7fce0389512d6a9361301708b92dff667/2020-08-11T21:36:48.204330Z, JVM: 14.0.1
# 修改yml配置文件,": "后面必须有空格
[root@cdh1 ~]$ vim config/elasticsearch.yml
cluster.name: my-es                     # 集群名称
node.name: node-1                       # 节点名称
network.host: cdh1                      # ip地址
http.port: 9200                         # 端口默认9200
discovery.seed_hosts: ["cdh1", "cdh2"]  # 自发现配置,新节点向集群报到的主机名
# 调整jvm启动内存大小
[root@cdh1 ~]$ vim config/jvm.options
-Xms1g
-Xmx1g
# 安装ik中文分词器,必须放到es的plugins目录下,分词就是将所有词汇分好放到elasticsearch-7.9.0/plugins/ik/config下的文件中
[root@cdh1 ~]$ unzip elasticsearch-analysis-ik-7.9.0.zip -d /opt/module/elasticsearch-7.9.0/plugins/ik
# 自定义词库,有时候词库并不包含项目中的专业术语或网络用语,可以在本地或远程扩展词库
[root@cdh1 ~]$ vim plugins/ik/config/IKAnalyzer.cfg.xml
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!--用户可以在这里配置自己的扩展字典 -->
    <entry key="ext_dict">./myword.txt</entry>
    <!--用户可以在这里配置自己的扩展停止词字典-->
    <entry key="ext_stopwords"></entry>
    <!--用户可以在这里配置远程扩展字典 -->
    <!-- <entry key="remote_ext_dict">words_location</entry> -->
    <!--用户可以在这里配置远程扩展停止词字典-->
    <!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
# 分发并修改node.name和network.host
[root@cdh1 ~]$ xsync /opt/module/elasticsearch-7.9.0
# 启动
[root@cdh1 ~]$ bin/elasticsearch
# 关闭
[root@cdh1 ~]$ ps -ef | grep -i 'elastic' | grep -v grep | awk '{print $2}' | xargs kill
# 测试连接
[root@cdh1 ~]$ curl http://localhost:9200/_cat/nodes?v
ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
127.0.0.1           19          98  10    2.65                  mdi       *      node-1
# 查看日志
[root@cdh1 ~]$ tail -f /opt/module/elasticsearch-7.9.0/logs/my-es.log

# 安装kibana
[root@cdh1 ~]$ tar -xvf kibana-7.9.0-darwin-x86_64.tar.gz -C /opt/module/
# 修改yml配置文件
[root@cdh1 ~]$ vim config/kibana.yml
server.host: "0.0.0.0"  # 授权远程访问
server.port: 5601       # 端口默认5601
elasticsearch.hosts: ["http://cdh1:9200", "http://cdh2:9200"]  # 指定es集群地址,逗号分隔
# 启动(要先启动es)
[root@cdh1 ~]$ bin/kibana
# 关闭
[root@cdh1 ~]$ ps -ef | grep -i 'kibana' | grep -v grep | awk '{print $2}' | xargs kill
# 测试连接
[root@cdh1 ~]$ http://localhost:5601/app/dev_tools#/console

# 一键启动es集群
[root@cdh1 ~]$ vim es.sh & chmod +x es.sh
#!/bin/bash 
es_home=/opt/module/elasticsearch-7.9.0
kibana_home=/opt/module/kibana-7.9.0-darwin-x86_64
case $1 in
"start"){
    for i in cdh1 cdh2 cdh3
    do
        echo "=============== ${i}启动es ==============="
        ssh ${i} "source /etc/profile && ${es_home}/bin/elasticsearch > /dev/null 2>&1 &"
    done
    echo "=============== 启动kibana ==============="
    nohup ${kibana_home}/bin/kibana > ${kibana_home}/logs/kibana.log 2>&1 &
};;
"stop"){
    echo "=============== 停止kibana ==============="
    ps -ef | grep kibana | grep -v grep | awk '{print \$2}' | xargs kill
    for i in cdh1 cdh2 cdh3
    do
        echo "=============== ${i}停止es ==============="
        ssh ${i} "ps -ef | grep -i 'elastic' | grep -v grep | awk '{print \$2}' | xargs kill" > /dev/null 2>&1
    done
};;
esac
```

### rest
```shell script
# web：分布式系统为互联网资源提供访问入口,url定位资源,rest就是用http动作对资源做crud操作
# rest(Representational State Transfer)：资源的表述性状态转移,是web服务的一种风格不是标准,符合rest原则的架构就称之为restful
# Representational资源的表现形式,比如json/xml/jpeg | State Transfer资源的状态变化,通过get/post/put/delete等http动词实现
# url的作用就是用来定位具体资源,url本身不应该包含/get这样的动词,具体的crud操作通过http协议的动词实现
# soap面向服务
Get http://localhost/QueryUser
Get http://localhost/CreateUser
Get http://localhost/ModifyUser
Get http://localhost/DeleteUser
# rest面向资源
Get http://localhost/user
POST http://localhost/user
PUT http://localhost/user
DELETE http://localhost/user
```

### es
```shell script
# elasticsearch是基于Lucene的企业级搜索引擎,是一个近实时的搜索平台,基于简单的restful api隐藏lucene的复杂性,让全文搜索变得简单
# kibana可以搜索存储在es索引中的数据并与之交互,实现数据分析和可视化并以图表形式展现,kibana本身只是一个工具,不涉及集群并发量也不大
cluster
# 集群：es默认是集群状态,哪怕只有一个节点,cluster.name参数设置集群唯一名称标识,节点都是通过该名称加入集群
node
# 节点：存储数据,参与集群的索引和搜索功能,es启动时会给节点分配一个UUID,集群包含一个或多个节点
index
# 索引：相当于mysql的table和mongodb的collection,可以crud
document
# 文档：相当于mysql的row,用json表示,一个索引包含多个文档
field
# 字段：相当于mysql的column,一个文档包含多个字段
shards & replicas
# 索引会存储大量数据,假设10亿文档将占用1TB磁盘,可能会超过单节点硬件限制,因此es提供了索引分片功能,每个分片都是功能齐全的独立索引
# 索引可以分片,分片可以创建副本,es创建索引时默认分配一个分片(主分片)和一个副本(副分片),可通过_shrink和_split动态更改(不建议)
# 集群中可以将主分片和副本分片放到不同节点,分片(节点)故障时可以提高可用性,并发搜索多个分片可以提高吞吐量

# es对比mysql
public class Movie {String id; String name; Double score; List<Actor> actorList;}
public class Actor {String id; String name;}
# 这两个java实体类如果保存在mysql会被拆成2张表,但是es可以用一个json来表示一个document
{"id": "1", "name": "red sea", "score": "8.5", "actorList": [{"id": "1", "name": "aaa"}, {"id": "3", "name": "bbb"}]}
```

### RestFulApi(DSL)
```shell script
# DSL(Domain Specific Language) 特定领域专用语言
# 全局操作,?v显示头信息
GET _cat/indices?v  # 查看默认索引,health/status/index/uuid/pri/rep/docs.count/docs.deleted/store.size/pri.store.size
GET _cat/health?v   # 查看集群健康状况,green正常 yellow单点正常 red故障
GET _cat/nodes?v    # 查看节点状况
GET _cat/shards?v   # 查看所有索引分片情况,/shards/${movie_index}可以指定索引
GET _cat/{...}?v    # 其他系统默认信息
# 中文分词
GET _analyze {"text": ["hello world"]}                      # 英文默认分词是空格, "hello" "world"
GET _analyze {"text": ["小菜鸟"]}                            # 中文默认分词是汉字,没有词汇概念需要分词器, "小" "菜" "鸟"
GET _analyze {"analyzer": "ik_smart", "text": ["小菜鸟"]}    # ik_smart分词方式, "小" "菜鸟"
GET _analyze {"analyzer": "ik_max_word","text": ["小菜鸟"]}  # ik_max_word分词方式, "小菜" "菜鸟"

# 索引操作
PUT/GET/DELETE ${index_name}  # 创建/查看/删除索引,索引必须是小写字母,es不支持修改索引结构
GET _cat/aliases?v            # 查看索引别名
POST _aliases                 # add添加别名,remove删除别名,应用场景：给相似的索引起相同别名,达到分组目的,通过别名查询组内所有索引
#{"actions": [{"add": {"index": "${index_name}","alias": "${index_name_alias}"}}]}
#{"actions": [{"remove": {"index": "${index_name}","alias": "${index_name_alias}"}}]}
GET ${index_name}/_mapping    # 查看索引的mapping,用于定义文档中的字段类型,创建索引时如果不设定mapping,系统会自动推断字段类型
PUT ${index_name}             # 创建索引时指定mapping,并对中文字符串添加ik分词器
#{"mappings": {"properties": {"id":{"type": "long"},"name":{"type": "text", "analyzer": "ik_smart"},
#"doubanScore":{"type": "double"},"actorList":{"properties": {"id":{"type":"long"},"name":{"type":"keyword"}}}}}}
POST _reindex                 # 索引拷贝,由于es不支持动态修改mapping,可以通过_reindex将一个索引的快照数据copy到另一个索引
#{"source": {"index": "${index_name}"}, "dest": {"index": "${index_name_new}"}}
GET _cat/templates?v          # 查看索引模板
PUT/GET _template/${template} # 创建/查看模板,应用场景：根据时间间隔(日/月/年)分割业务索引,可以随时间灵活改变索引结构并且可以优化查询范围
#{"index_patterns": ["order*"], "settings": {"number_of_shards": 1}, "aliases": {"{index}-query": {}, "order-query":{}},
#"mappings": {"properties": {"id": {"type": "keyword"}, "name": {"type": "text","analyzer": "ik_smart"}}}}

# 文档操作
PUT/GET/DELETE ${index_name}/_doc/${doc_id}  # 创建/查看/删除文档,如果doc_id已存在直接覆盖(幂等性)
#{"id":100, "name":"red sea", "doubanScore":8.5, "actorList":[{"id":1,"name":"zhang yi"},{"id":2,"name":"hai qing"}]}
GET ${index_name}/_search?pretty             # 查看索引所有文档
POST ${index_name}/_doc                      # 生成随机id创建文档(非幂等性)
POST ${index_name}/_update/${doc_id}         # 更新文档,doc是固定写法
#{"doc": {"name": "red sea"}}
POST ${index_name}/_bulk                     # _bulk表示批处理,批量操作的json要写在同一行
#{"index":{"_id":11}}
#{"id":100,"name":"red sea","doubanScore":5.0,"actorList":[{"id":4,"name":"aaa"}]}
#{"index":{"_id":12}}
#{"id":200,"name":"blue sea","doubanScore":5.0,"actorList":[{"id":4,"name":"bbb"}]}
#{"update": {"_id": "11"}}
#{"doc": {"name": "green sea"}}
#{"delete": {"_id": "12"}}

# 条件查询
GET ${index_name}/_search
# match_all查询全部
{"query": {"match_all": {}}}
# match分词查询,es字符串有text(分词)和keyword(不分词)两种类型,分词会命中多个文档但分值不同
{"query": {"match": {"name": "sea"}}}
# match分词子属性查询
{"query": {"match": {"actorList.name": "hai"}}}
# match_phrase短语查询,相当于%like%
{"query": {"match_phrase": {"actorList.name": "han yu"}}}
# term精准查询,字符串要选keyword类型
{"query": {"term": {"name.keyword": {"value": "operation red sea"}}}}
# fuzzy容错匹配,当分词查询匹配不到准确结果时会匹配接近的单词
{"query": {"fuzzy": {"name": "rivor"}}}
# range范围查询,gt> lt< gte>= lte<=
{"query": {"range": {"doubanScore": {"gte": 8.5,"lte": 9.5}}}}
# bool查询时过滤
{"query": {"bool": {"must": [{"match": {"name": "sea"}}],"filter": [{"term": {"actorList.id": "1"}}]}}}
# sort查询后排序
{"query": {"match": {"name": "sea"}},"sort": [{"doubanScore": {"order": "desc"}}]}
# from和size分页查询
{"query": {"match_all": {}},"from": 0,"size": 2}
# _source只显示指定字段
{"query": {"match_all": {}},"_source": ["name","doubanScore"]}
# highlight高亮显示
{"query": "match": {"name": "red sea"}},"highlight": {"fields": {"name": {}}}}
# aggs聚合操作,terms/sum/max/avg/value_count等对数据分组统计,同时返回搜索结果和聚合结果,强大且高效,聚合不需要分词所以字符串要选keyword类型
{"aggs": {"groupBy_actor_score_sort": {"terms": {"field": "actorList.name.keyword","size": 10,
"order": {"avg_score": "asc"}},"aggs": {"avg_score": {"avg": {"field": "doubanScore"}}}}}}
```

|mysql   |mongodb   |es      |说明|
| :---:  | :---:    | :---:  | :---:
|database|db        |-       |数据库|
|table   |collection|index   |表/集合/索引|
|row     |document  |document|行/文档|
|column  |field     |field   |列/字段|
|index   |index     |-       |索引|
|join    |-         |-       |nosql不维护表之间的关系|
|p-key   |p-key     |-       |mongo自动设置_id为主键|

|         | mysql  |redis| es   | hbase| hive|
| :---:   | :---:  |:---:|:---: | :---:|:---:|
|容量      |中等     |小   |较大  |海量  | 海量  |
|查询时效性 |中等     |极快  |较快  |中等  | 慢   |
|查询灵活性 |很好(sql)|较差  |较好  |较差  |很好(sql)|
|写入速度   |中等     |极快 |较快  |较快  | 慢   |
|一致性(事务)|Y       |N   |N    |N    | N    |